


@article{Garber:2015td,
author = {Garber, Dan and Hazan, Elad},
title = {{Fast and Simple PCA via Convex Optimization}},
journal = {arXiv.org},
year = {2015},
eprint = {1509.05647v4},
eprinttype = {arxiv},
eprintclass = {math.OC},
month = {sep}
}



@article{Allen2015UniVR,
  title={UniVR: A universal variance reduction framework for proximal stochastic gradient method},
  author={Allen-Zhu, Zeyuan and Yuan, Yang},
  journal={arXiv preprint arXiv:1506.01972},
  year={2015}
}

@article{Shah2016Trading,
  title={Trading-off variance and complexity in stochastic gradient descent},
  author={Shah, Vatsal and Asteris, Megasthenis and Kyrillidis, Anastasios and Sanghavi, Sujay},
  journal={arXiv preprint arXiv:1603.06861},
  year={2016}
}

@inproceedings{ShalevShwartz:2016vy,
author = {Shalev-Shwartz, Shai},
title = {{SDCA without duality, regularization, and individual convexity}},
booktitle = {International Conference on Machine Learning},
year = {2016},
address = {New York, USA},
month = {June},
annote = {International Conference on Machine Learning 2016}
}




@inproceedings{AllenZhu:2016up,
author = {Allen-Zhu, Zeyuan and Hazan, Elad},
title = {{Variance reduction for faster non-Convex optimization}},
booktitle = {International Conference on Machine Learning},
year = {2016},
address = {New York, USA},
month = {June}
}



@article{Johnson:9MAvkbgy,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, R and Zhang, T},
  journal={Advances in Neural Information Processing Systems},
  pages={315--323},
  address={Lake Tahoe, USA},
  month={December},
  year={2013}
}

@article{Richtarik:2013te,
  title={Semi-stochastic gradient descent methods},
  author={Kone{\v{c}}n{\`y}, Jakub and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1312.1666},
  year={2013}
}


@article{Liu:2015bx,
  title={Mini-batch semi-stochastic gradient descent in the proximal setting},
  author={Kone{\v{c}}n{\`y}, Jakub and Liu, Jie and Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={10},
  number={2},
  pages={242--255},
  year={2016},
  publisher={IEEE}
}

@inproceedings{Zhang2013Linear,
  title={Linear convergence with condition number independent access of full gradients},
  author={Zhang, Lijun and Mahdavi, Mehrdad and Jin, Rong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={980--988},
  address={Lake Tahoe, USA},
  month={December},
  year={2013}
}

@inproceedings{Li:2016vh,
author = {Li, Xingguo and Zhao, Tuo and Arora, Raman and Liu, Han and Haupt, Jarvis},
title = {{Stochastic variance reduced optimization for nonconvex sparse learning}},
booktitle = {International Conference on Machine Learning},
year = {2016},
month = {June},
address = {New York, USA},
}


@article{Xiao:2014vw,
  title={A proximal stochastic gradient method with progressive variance reduction},
  author={Xiao, Lin and Zhang, Tong},
  journal={SIAM Journal on Optimization},
  volume={24},
  number={4},
  pages={2057--2075},
  year={2014},
  publisher={SIAM}
}


@article{Allenzhu2016Katyusha,
  title={Katyusha: accelerated rariance reduction for faster SGD},
  author={Allen-Zhu, Zeyuan},
  journal={arXiv preprint arXiv:1603.05953},
  year={2016}
}

@inproceedings{Allen2015Improved,
author = {Allen-Zhu, Zeyuan and Yuan, Yang},
title = {{Improved SVRG for non-strongly-convex or sum-of-non-convex objectives}},
booktitle = {International Conference on Machine Learning},
year = {2016},
address = {New York, USA},
month = {June},
}



@inproceedings{Defazio:2014vu,
  title={Saga: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in Neural Information Processing Systems},
  month={December},
  address={Montr{\'e}al, Canda},
  pages={1646--1654},
  year={2014}
}

@article{Schmidt:2013ui,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Roux, Nicolas Le and Bach, Francis},
  journal="Mathematical Programming",
  year="2016",
  pages="1--30"
}

@article{KolteAccelerating,
  title={Accelerating SVRG via second-order information},
  author={Kolte, Ritesh and Erdogdu, Murat},
  year={2016}
}

@article{Tan2016Barzilai,
  title={Barzilai-Borwein Step Size for Stochastic Gradient Descent},
  author={Tan, Conghui and Ma, Shiqian and Dai, Yu Hong and Qian, Yuqiu},
  journal={arXiv preprint arXiv:1605.04131},
  year={2016},
}

@article{Barzilai1988Two,
  title={Two-Point Step Size Gradient Methods},
  author={Barzilai, Jonathan and Borwein, Jonathan M.},
  journal={IMA Journal of Numerical Analysis},
  volume={8},
  number={1},
  pages={141-148},
  year={1988},
}

@article{Robbins1951A,
  title={A Stochastic Approximation Method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={Annals of Mathematical Statistics},
  volume={22},
  number={3},
  pages={400-407},
  year={1951},
}

@article{Boyd2013Convex,
  title={Convex Optimization},
  author={Boyd and Vandenberghe and Faybusovich},
  journal={IEEE Transactions on Automatic Control},
  volume={51},
  number={11},
  pages={1859-1859},
  year={2013},
}

@article{Dean2012Large,
  title={Large Scale Distributed Deep Networks},
  author={Dean, Jeffrey and Corrado, Greg S and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V and Mao, Mark Z and Ranzato, Aurelio and Senior, Andrew and Tucker, Paul},
  journal={Advances in Neural Information Processing Systems},
  pages={1232-1240},
  year={2012},
}

@article{Roux2012A,
  title={A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets},
  author={Roux, Nicolas Le and Schmidt, Mark and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={4},
  pages={2663-2671},
  year={2012},
}

@article{Sa2015Global,
  title={Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix Problems},
  author={Sa, Christopher De and Olukotun, Kunle and RÃ©, Christopher},
  journal={Mathematics},
  pages={2332-2341},
  year={2015},
}


